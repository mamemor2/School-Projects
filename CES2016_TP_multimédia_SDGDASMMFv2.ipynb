{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP donnéees multimédia - CES2016\n",
    "Stéphane DONNE    \n",
    "Guillaume DANJOU    \n",
    "Arthur SECK   \n",
    "Mame-Mor FALL    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectifs du TP :**\n",
    "O1 définir ce que sont les données multimedia (terme multimedia, qui a plusieurs assertions + exemples\n",
    "variés, enjeux liés aux masses de données)\n",
    "O2 extraire des caractéristiques propres aux données sonores (pitch, MFCC, etc. ) et visuelles (couleur,\n",
    "forme, texture, notamment SIFT et \"sacs de mots\" et caractéristiques propres aux visages). Attention il\n",
    "faudra sans doute revoir quelques bases (Transformée de Fourier, notion de fréquence)\n",
    "O3 mesurer l'intérêt de ces caractéristiques ; savoir poser le problème plus que donner toutes les\n",
    "réponses : suggestions de quelques heuristiques et ouverture vers la sélection automatique ; aspects\n",
    "subjectifs (notion de fossé sémantique)\n",
    "O4 réutiliser de façon autonome ces savoirs dans un projet personnel\n",
    "\n",
    "● synthèse des connaissances acquises dans ce module (détaillées) ;\n",
    "● discussion critique de leur mise en œuvre sur l’activité vidéo (démonstration d’une réflexion\n",
    "personnelle face à des problèmes concrets) ;\n",
    "● description de la mise en œuvre dans le projet personnel (éventuellement) ;\n",
    "● 1 question que l’on aurait aimé approfondir (pour prendre du recul)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O1 définir ce que sont les données multimedia (terme multimedia, qui a plusieurs assertions + exemples variés, enjeux liés aux masses de données) :**\n",
    "\n",
    "Les \"données multimédia\" au sens large sont l'ensemble des informations que l'on peut extraire de contenus audiovisuels. Les exemples de contenus audiovisuels et de cas d'analyses n'ont de limite que l'imagination humaine  :\n",
    "- Transcrire un texte à partir de signaux de paroles humaine (speech to text translation, ou traduire en temps réel les paroles dans une autre langue speech to speech translation comme proposé par Skype)\n",
    "- Retrouver le titre et les auteurs à partir d'une bande sonore musicale (Shazam)\n",
    "- Identifier des visages dans une image (appareils photo, Facebook propose d'identifier automatiquement les visages de nos amis sur les photos, Google Photos propose de classer les albums par personne identifié etc.)\n",
    "- Identifier des actions dans une vidéo (travaux de recherche en cours ex:http://lear.inrialpes.fr/)\n",
    "- etc.\n",
    "\n",
    "Aujourd'hui ces contenus dont la disponibilité explose sur Internet sont l'objet d'enjeux collosaux pour leur stockage et leur accès rapide premièrement, et pour leur analyse deuxièmement afin de proposer de la valeur commerciale par l'automatisation de tâche ou la recommandation de contenus.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O2 extraire des caractéristiques propres aux données sonores (pitch, MFCC, etc. ) et visuelles (couleur, forme, texture, notamment SIFT et \"sacs de mots\" et caractéristiques propres aux visages). Attention il faudra sans doute revoir quelques bases (Transformée de Fourier, notion de fréquence) :**\n",
    "\n",
    "L'objectif que l'on se donne dans cette séquence est d'arriver à retrouver plusieurs séquences à l'intérieur d'un enregistrement de l'émission vidéo \"le débat\".\n",
    "- les transitions musique/parole\n",
    "- les différents locuteurs au sein de l'émission\n",
    "\n",
    "Extraction de la bande sonore :\n",
    "La bande sonore peut être extraite par l'intermédiaire de plusieurs logiciels libres disponibles (ex: ffmpeg). Pour ce TP elle nous est déjà donnée en séance en format WAVE (Waveform Audio File Format) stéréo échantillonné à 44100 bit/secondes.\n",
    "\n",
    "Nous pouvons donc directement commencer à analyser la bande sonore au travers de python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La stratégie consiste à exploiter les coefficients cespraux tirés du signal sonore et à les utiliser pour construire un modèle de classification. La base d'apprentissage sera composée des premières minutes d'enregistrement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le signal sonore est stocké dans un numpy array. On utilise la bibliothèque librosa. Pour que le programme tourne rapidement, on se limite à 10 minutes d'enregistrement avec le paramètre \"duration\", mais on pourrait faire tourner le programme sur la durée entière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-1-ee49030227a3>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-ee49030227a3>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    print \"la frequence d'echantillonnage est\", sr\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Sun Apr  3 17:18:11 2016\n",
    "\n",
    "@author: Stéphane, Guillaume, Arthur, Mame-Mor\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "\n",
    "%matplotlib inline\n",
    "duration=180\n",
    "y, sr = librosa.core.load(\"/home/reddowan/Documents/Telecom Paris/4-Multimedia/06-11-22.wav\", sr=44100, duration=duration)\n",
    "print \"la frequence d'echantillonnage est\", sr\n",
    "print \"la taille de l'array y est de\", y.shape\n",
    "librosa.display.waveplot(y,sr)\n",
    "plt.title('Amplitude de la bande sonore')\n",
    "plt.xlabel('temps')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "\"\"\"On observe la representation des variations d'amplitude de la bande sonore en fonction du temps\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = scipy.fft(y)\n",
    "z_mag=np.absolute(z)\n",
    "plt.figure()\n",
    "plt.plot(z_mag)\n",
    "plt.title('Amplitudes spectrales')\n",
    "plt.xlabel('frequences')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après ces deux étapes de visualisation du signal nous allons construire une matrice de caractéristiques du signal. Cette matrice contiendra en colonne les coefficients cespraux calculés à l'aide de l'algorithme MFCC (representation filtre-source de la bande spectrale basée sur la psychoacoustique qui permet une bonne discrimination des signaux de paroles) et le Zero Cross Rate (taux de passage par zéro du signal qui est une dimension discriminante pour la musique et la voix). \n",
    "\n",
    "On normalisera ces différents paramètres avant d'essayer plusieurs algorithmes pour classifier les différentes séquences.\n",
    "On essaiera un algorithme de classification non supervisé, le clustering par K-means.\n",
    "Puis on entraînera un classifieur supervisé le LDA.\n",
    "Enfin nous comparerons les résultats et élaborerons sur ce que nous aurions aimé faire pour aller plus loin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la matrice de features\n",
    "### Calcul du ZCR\n",
    "Avec le ZCR on espère obtenir un paramètre efficace pour séparer les séquences de paroles des séquences de musiques. Le ZCR se calcule sur une fenêtre glissante qui a une taille (frame_length) et glisse de n échantillons entre deux itérations (hop_length). Les mêmes paramètres seront utilisés pour le calcul des coefficients cepstraux avec le MFCC. Le choix de ces paramètres est important car on comprend intuitivement qu'une fenêtre trop grande moyenne les évolutions du signal, on pourrait ainsi avoir une fenêtre qui comprend toujours plusieurs séquences à la fois et dans ce cas impossible d'avoir une discrimination efficace. Dans le sens inverse une fenêtre trop petite  entraînera du bruit car nous seront ammènés à voir des changements de séquence à un niveau de détail qui ne nous intéresse pas. Idéalement nous voulons isoler 6 séquences, 5 locuteurs (1 présentatrice + 4 intervenants) et la musique de l'emission.\n",
    "Pour trouver les paramètres de fenêtre optimaux nous pourrions en tester plusieurs et trouver ceux qui isole le mieux ces 6 séquences mais cela serait très consommateur de ressources. Nous essaierons donc arbitrairement une fenêtre d'1 seconde et un hop length de 0.5 seconde qui nous semble les performances qu'un humain aurait a discriminer les différentes séquences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fft=44100\n",
    "hop_length=22050\n",
    "sr=44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_crossing_rate = librosa.feature.zero_crossing_rate(y, frame_length=n_fft, hop_length=hop_length)\n",
    "plt.plot(zero_crossing_rate[0])\n",
    "plt.title('Zero Crossing Rate')\n",
    "plt.xlabel('Echantillons (par demi-seconde)')\n",
    "plt.ylabel('Taux')\n",
    "zero_crossing_rate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUr le graphique ci-dessus on observe bien une différence de pattern entre les 10 premières secondes de musique et le reste du signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du MFCC\n",
    "Avec les coefficients MFCC on souhaite séparer les différents locuteurs au sein des signaux de parole. Comme le ZCR, le melspectrogram se calcule sur une fenêtre glissante dont nous conserverons les paramètres utilisés précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S=librosa.feature.melspectrogram(y=y,sr=sr,n_fft=n_fft,hop_length=hop_length)\n",
    "print S.shape\n",
    "logS=librosa.logamplitude(S)\n",
    "librosa.display.specshow(logS[0:13,:],sr=sr,y_axis='mel')\n",
    "plt.title('Representation du melspectrogram')\n",
    "plt.show()\n",
    "\"\"\"Zoom sur les 13 premiers coefficients cepstraux\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On agrège les paramètres calculés en une matrice des paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Features=np.concatenate((zero_crossing_rate,logS[0:13,:]),axis=0)\n",
    "print (Features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation des données\n",
    "On normalise les 14 paramètres de notre matrice pour améliorer les résultats de notre future classification et éviter le surapprentissage sur quelques paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Features_norm = scaler.fit_transform(Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering des séquences\n",
    "Dans cette section nous allons essayer de produire la segmentation de l'extrait en différente séquence de manière non-supervisée. C'est à dire que nous n'allons pas indiquer à l'algorithme les labels recherchés et leurs caractéristiques par de l'apprentissage. L'algorithme de clustering va séparer l'extrait sonore en différentes parties les plus \"proches\" au sens de la distance dans l'espace de représentation. Plusieurs algorithme de clustering existe, celui proposé par librosa s'appuie sur la librairie scikit learn et s'appelle agglomerative clustering. Dans le cas de 2 classes, cet algorithme parcourt l'extrait sonore pour trouver la séparation temporelle qui minime la distance de chacune des observations de caractéristiques au centre de classe ainsi déterminé. Idem avec plusieurs classes. L'idée clé est que les observations de classes sont continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boundary_frames=librosa.segment.agglomerative(Features_norm,2)\n",
    "librosa.frames_to_time(boundary_frames,sr=sr)\n",
    "print(boundary_frames*500)\n",
    "librosa.display.waveplot(y,sr,x_axis='time')\n",
    "plt.vlines(boundary_frames*500, ymin=-1,ymax=1, color='r', linestyles='solid', label='Segment boundaries')\n",
    "plt.legend(frameon=True, shadow=True)\n",
    "plt.title('Waveplot et cluster')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier résultat est très intéressant. On constate en vérifiant sur l'enregistrement sonore que les deux séquences séparées par l'algorithme correspondent bien à la disparition complète de la musique en fond sonore pour marque le début de l'émission où la présentatrice prend la parole. En réalité un auditeur humain aurait marqué la fin de la musique aux alentours des 10 secondes quand l'introduction commence, mais durant tout le reportage la musique continue en arrière plan. Avec notre normalisation de la matrice, l'intensité des coefficients n'entre plus en compte et ce résultat semble tout à fait cohérent.\n",
    "\n",
    "On ajoute un cluster supplémentaire et on observe le résultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boundary_frames=librosa.segment.agglomerative(Features_norm,3)\n",
    "librosa.frames_to_time(boundary_frames,sr=sr)\n",
    "print(boundary_frames*500)\n",
    "librosa.display.waveplot(y,sr,x_axis='time')\n",
    "plt.vlines(boundary_frames*500, ymin=-1,ymax=1, color='r', linestyles='solid', label='Segment boundaries')\n",
    "plt.legend(frameon=True, shadow=True)\n",
    "plt.title('Waveplot et cluster')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons à nouveau un résultat parfaitement interprétable, la nouvelle séparation identifiée par l'algorithme correspond à la fin du jingle de début d'émission et au démarrage du reportage d'introduction. Ceci valide la pertinence d'une telle approche pour identifier différentes séquences dans un extrait sonore, de plus l'avantage d'une méthode non-supervisée est sa généralisation possible à d'autres émissions par exemple, si tenté que nous ayons l'hyperparamètre k du nombre de séquences recherchées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage supervisé par Linear Discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser un apprentissage supervisé nous devons créer des étiquettes cibles que nous tacherons de retrouver sur des nouvelles données qui n'ont pas été utilisée pour l'apprentissage. Pour cette partie l'objectif va être de retrouver le genre du locuteur (Homme ou Femme), on crée donc un numpy array, nommé genre, contenant le genre du locuteur à chaque instant. Pour retrouver facilement dans ce numpy array genre un instant de l'enregistrement sous forme de minute et seconde, deux paramètres minute et seconde sont alimentés.\n",
    "\n",
    "Les caractéristiques que nous utiliserons pour cette partie seront les coefficients cesptraux obtenus par MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z, sr = librosa.core.load(sr = 44100, path='D:/Formations/Telecom_ParisTech/Module_04/06-11-22.wav', duration=10*60)\n",
    "Z = scipy.fft(z)\n",
    "Z_mag=np.absolute(Z)\n",
    "S=librosa.feature.melspectrogram(z,sr=sr)\n",
    "logS=librosa.logamplitude(S)\n",
    "genre=np.zeros(len(logS.T))\n",
    "X=np.zeros(len(logS.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coef_cresp_minute_seconde(coef_min,coef_max,minute_debut, seconde_debut,minute_fin,seconde_fin):\n",
    "    librosa.display.specshow(logS[coef_min:coef_max,minute*minute_debut+seconde*seconde_debut:minute*minute_fin+seconde*seconde_fin],sr=sr,x_axis='time',y_axis='mel')\n",
    "    plt.title('Representation du melspectrogram')\n",
    "    plt.show()\n",
    "    print(coef_min)\n",
    "    print(coef_max)\n",
    "    print(minute*minute_debut+seconde*seconde_debut)\n",
    "    print(minute*minute_fin+seconde*seconde_fin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le melspectrogramme est tracé entre les temps 1mn00s et 4 mn 58s de l'extrait sonore. On semble percevoir la différence entre la séquence de 1 à 2 mn 59 s pendant laquelle la femme parle et la minute suivante où l'homme parle. Une autre femme parle entre 4 mn et 4 mn 58 s, le melspetgrogram est \"proche\" de celui de la première femme qui a parlé.\n",
    "\n",
    "Cela laisse penser que les coefficients cesptraux permettront peut-être de différencier les hommes et les femmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_cresp_minute_seconde(0,13,0,0,4,59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fournit en apprentissage les données de genre des minutes 1 à 5 de l'enregistrement. On tire parti du fait que, pendant ces moments, on entend distinctement une personne à la fois pendant un temps assez long. Le défaut pourrait être que le modèle \"n'apprendrait pas bien\" les séquences pendant lesquelles plusieurs personnes parlent en même temps, ainsi que les prises de parole très brèves.\n",
    "On annote cette matrice de genre manuellement pour l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre[minute*1+seconde*0:minute*3+seconde*0]=2         # une femme parle entre 1 mn 00s et 3 mn 00s (genre=2)\n",
    "genre[minute*3+seconde*0:minute*3+seconde*52]=1        # un homme parle entre 3 mn 00s et 3 mn 52s  (genre=1)\n",
    "genre[minute*3+seconde*52:minute*4+seconde*44]=2\n",
    "genre[minute*4+seconde*44:minute*4+seconde*59]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le genre du locuteur est tracé pendant les 5 premières minutes. La première minute (valeur 0) ne sera pas prise en compte, car elle contient de la musique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.ylim(0,3)\n",
    "plt.plot(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(logS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée les données pour l'apprentissage. Le numpy array X contient les n premiers coefficients cesptraux pendant la période 1mn00s à 4mn59s. y contient le genre du locuteur pendant cette période (de manière à travailler avec les notations habituelles X pour les features et y pour l'étiquette de classification)\n",
    "\n",
    "Le numpy array \"X_reste\" contient le reste de l'enregistrement (de 5mn00s à 10mn00) pour tester le bon fonctionneemnt du modèle au-delà de la base de test (X_test, voir plus loin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_coef_cespraux = 13   #paramètres permettant de sélectionner les n premiers coefficients cespraux\n",
    "\n",
    "X=logS.T[minute*1+seconde*0:minute*4+seconde*59,0:nb_coef_cespraux-1]\n",
    "y = genre[minute*1+seconde*0:minute*4+seconde*59]\n",
    "print(X.shape, y.shape, np.unique(y))\n",
    "\n",
    "X_reste=logS.T[minute*5+seconde*0:51680,0:nb_coef_cespraux-1]\n",
    "print(X_reste.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y)\n",
    "plt.ylim(0,3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données X sont standardisées pour améliorer les résultats du classifieur et éviter le surapprentissage sur une partie des coefficients de plus forte amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Xnorm = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue une partition de X et y entre la partie des données que nous allons utiliser pour l'apprentissage (75 % de l'ensemble nommés X_train et y_train) et la partie que nous gardons pour le test (25 % de l'ensemble nommés X_test et y_test). Cette partition peut présenter un inconvénient si elle conduisait à retenir un trop petit nombre d'observation pour les hommes ou les femmes.\n",
    "On pourrait pallier ce problème en s'assurant d'avoir des échantillons représentatif des deux genres en utilisant y. Nous ne le faisons pas ici, mais cela deviendrait très important si on essayait d'identifier le locuteur par exemple et non plus seulement son genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xnorm,y,train_size=0.75,random_state=20)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LDA.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(np.mean(cross_val_score(LDA,X_train,y_train,cv=5)))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient un taux de prédiction de 89 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la prévision sur la partie restante de l'enregistrement (5mn 00 à 9 mn 59s) pour aller au-delà de la base de test. Le but est de simuler ce qui se passerait si le classifieur était mis \"en production\". Les features de ces 5 minutes d'enregistrement supplémentaires sont stockées dans le numpy array X_reste (X_reste_norm pour les données standardisées). Y_reste contient la valeur du genre pour les minutes 5mn00s à 9mn59s d'enregistrement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_reste_norm = scaler.transform(X_reste)\n",
    "print(X_reste_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_reste_norm = scaler.transform(X_reste) # on applique la strandardisation utilisée sur l'échantillon train\n",
    "y_reste = LDA.predict(X_reste_norm)           # prédiction du genre\n",
    "\n",
    "print(X_reste_norm.shape)\n",
    "print(y_reste.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.ylim(0,3)\n",
    "plt.plot(y_reste)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problème apparaît immédiatement au vu du graphique ci-dessus. Les oscillations entre les valeurs prédites 1 et 2 sont grandes, y compris dans une même seconde (voir graphique ci-dessous avec la prédiction de la première seconde). Nous pensons que cela tient au caractère très fluctuant des caractéristiques temporelles du signal. Par voie de conséquence, les coefficients cespraux doivent être volatiles au cours du temps, d'où l'instabilité dans les résultats prédits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y_reste[86*0:86*5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une idée, provenant de l'analyse seconde par seconde des graphiques, est de voir qu'il y a bien des oscillations, mais que, \"la majorité du temps\", une valeur est prise pour le genre plutôt que l'autre. Nous tentons donc la règle suivante : si, au cours d'une seconde de temps donnée, le clasiffieur prédit 1 plus de la moitié du temps, on affecte 1 sur la seconde entière, 2 dans le cas contraire. Cela permettra de lisser les valeurs obtenues. Ce calcul est stocké dans un numpy array nommé genre_reste.\n",
    "\n",
    "Une autre approche serait d'élargir la fenètre glissante du calcul des coefficients cepstraux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre_reste = np.zeros(y_reste.shape[0])\n",
    "moyenne = np.zeros(y_reste.shape[0])\n",
    "\n",
    "for i in range(0,y_reste.shape[0],seconde):\n",
    "    moyenne[i]= np.mean(y_reste[i:i+seconde-1])\n",
    "    if moyenne[i] < 1.5:\n",
    "         genre_reste[i/seconde]=1\n",
    "    else:\n",
    "        genre_reste[i/seconde]=2     \n",
    "    #print(i,i+seconde-1, round(np.mean(y_reste[i:i+seconde-1]),2), genre_reste[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectivement, la figure ci-dessous (qui retrace les 5 minutes d'enregistrement entre 5mn00 et 9mn59s de l'émission) montre que le lissage a opéré. On voit en particulier une durée assez longue entre les secondes 70 et 150 (donc entre les temps 6mn10 et 7 mn 30s) pendant laquelle une femme parle (Juliette MATHYS-SIERRO, voir la vidéo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(genre_reste[0:300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec l'écoute de la bande son, on construit le tableau des \"vraies\" valeurs pour le genre (dans un numpy array nommé true_genre_reste). Cela permettra une comparaison graphique avec la prévision (numpy array genre_reste, voir plus haut) et le calcul du pourcentage de temps sur lequel le genre du locuteur est correctement prédit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tableau ci-dessous correspond à une annotation effectuée manuellement sur la base de l'écoute de l'enregistrement.\n",
    "\n",
    "Minute\tSeconde\tPersonne\tGenre\n",
    "5\t0\tJérôme MORISOD\tH\n",
    "5\t7\tIsabelle GAY\tF\n",
    "5\t9\tJérôme MORISOD\tH\n",
    "5\t15\tIsabelle GAY\tF\n",
    "5\t19\tJérôme MORISOD\tH\n",
    "5\t37\tIsabelle GAY\tF\n",
    "5\t42\tJérôme MORISOD\tH\n",
    "6\t6\tIsabelle GAY\tF\n",
    "6\t17\tJuliette MATHYS-SIERRO\tF\n",
    "6\t42\tIsabelle GAY\tF\n",
    "6\t56\tJuliette MATHYS-SIERRO\tF\n",
    "7\t22\tIsabelle GAY\tF\n",
    "7\t23\tJuliette MATHYS-SIERRO\tF\n",
    "7\t32\tIsabelle GAY\tF\n",
    "7\t35\tDamien KOENIG\tH\n",
    "8\t17\tIsabelle GAY\tF\n",
    "8\t39\tDamien KOENIG\tH\n",
    "8\t53\tIsabelle GAY\tF\n",
    "8\t55\tDamien KOENIG\tH\n",
    "9\t7\tIsabelle GAY\tF\n",
    "9\t8\tDamien KOENIG\tH\n",
    "9\t11\tIsabelle GAY\tF\n",
    "9\t14\tDamien KOENIG\tH\n",
    "9\t46\tIsabelle GAY\tF\n",
    "9\t48\tBrigitte BERTHOUZOZ\tF\n",
    "9\t58\tIsabelle GAY\tF\n",
    "10\t0\tBrigitte BERTHOUZOZ\tF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_genre_reste = np.zeros(y_reste.shape[0])\n",
    "true_genre_reste[0:7] = 1\n",
    "true_genre_reste[7:9] = 2\n",
    "true_genre_reste[9:15] = 1\n",
    "true_genre_reste[15:19] = 2\n",
    "true_genre_reste[19:37] = 1\n",
    "true_genre_reste[37:42] = 2\n",
    "true_genre_reste[42:66] = 1\n",
    "true_genre_reste[66:77] = 2\n",
    "true_genre_reste[77:102] = 2\n",
    "true_genre_reste[102:116] = 2\n",
    "true_genre_reste[116:142] = 2\n",
    "true_genre_reste[142:143] = 2\n",
    "true_genre_reste[143:152] = 2\n",
    "true_genre_reste[152:155] = 2\n",
    "true_genre_reste[155:197] = 1\n",
    "true_genre_reste[197:219] = 2\n",
    "true_genre_reste[219:233] = 1\n",
    "true_genre_reste[233:235] = 2\n",
    "true_genre_reste[235:247] = 1\n",
    "true_genre_reste[247:248] = 2\n",
    "true_genre_reste[248:251] = 1\n",
    "true_genre_reste[251:254] = 2\n",
    "true_genre_reste[254:286] = 1\n",
    "true_genre_reste[286:288] = 2\n",
    "true_genre_reste[288:298] = 2\n",
    "true_genre_reste[298:300] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule le pourcentage de secondes dans la séquence de 5 mn (allant de 5mn00 à 9mn59s) pour lesquelles le genre du locuteur est bien prévu. On obtient 93,3 %, ce qui semble relativement correct. Il convient toutefois d'analyser les échecs pour voir si cela correspond à un cas particulier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(100*np.sum(true_genre_reste[0:300]==genre_reste[0:300])/300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'examen des instants de mauvaise prédiction montre qu'il s'agit souvent du moment de \"raccord\" entre une voix féminine et une voix masculine. Il serait donc possible d'améliorer le taux de bonne prévision en annotant plus finement le genre du locuteur avec un temps mesuré en demi-secondes ou en dixièmes de secondes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.where(true_genre_reste[0:300]!=genre_reste[0:300])\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs pistes d'amélioration de ce travail sont possibles :\n",
    "1. Améliorer la qualité de prédiction par un réglage très fin des hyperparamètres par GridSearchCV\n",
    "2. Essayer de réduire la durée de la plage d'apprentissage tout en obtenant un taux de prédiction correcte supérieur à 90 %.\n",
    "3. Faire le lien entre ce module et le logiciel ELAN pour annoter automatiquement la vidéo.\n",
    "4. Etendre ce travail à la prédiction de l'identité du locuteur et non plus seulement du sexe.\n",
    "5. Déterminer si un autre type de classifieur (type KNN) n'aboutirait pas à de meilleures performances, voir ci-après."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage supervisé par KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KNN en choisissant k par validation croisée (V-fold) avec V = 6)\n",
    "from sklearn import neighbors\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "max_k = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
    "scores = np.empty(len(max_k))\n",
    "\n",
    "for k, d in enumerate(max_k):\n",
    "    clf_KNN_optim = neighbors.KNeighborsClassifier(n_neighbors=d)\n",
    "    scores[k] = np.mean(cross_val_score(clf_KNN_optim, X_train, y_train, cv=6, scoring='accuracy'))\n",
    "best_score = scores.argmax()\n",
    "best_k = max_k[best_score]\n",
    "print \"best k : \" + str(best_k)\n",
    "\n",
    "##Best KNN\n",
    "\n",
    "clfKNN = neighbors.KNeighborsClassifier(n_neighbors=best_k)\n",
    "clfKNN.fit(X_train, y_train)\n",
    "\n",
    "y_predKNN = clfKNN.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(np.mean(cross_val_score(clfKNN,X_train,y_train,cv=5)))\n",
    "print(confusion_matrix(y_test, y_predKNN))\n",
    "\n",
    "y_resteKNN = clfKNN.predict(X_reste_norm)           # prédiction du genre\n",
    "\n",
    "print(X_reste_norm.shape)\n",
    "print(y_resteKNN.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.ylim(0,3)\n",
    "plt.plot(y_resteKNN)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fonction getGenreReste est créée. Elle a pour but de déterminer chaque seconde le genre du locuteur à partir de la moyenne prédite par le classifieur calculée sur une seconde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGenreReste(x, rate):\n",
    "    output = np.zeros(x.shape[0])\n",
    "    moyenne = np.zeros(x.shape[0])\n",
    "\n",
    "    for i in range(0,x.shape[0],rate):\n",
    "        moyenne[i]= np.mean(x[i:i+rate-1])\n",
    "        if moyenne[i] < 1.5:\n",
    "             output[i/rate]=1\n",
    "        else:\n",
    "            output[i/rate]=2\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print scores\n",
    "genre_resteKNN = getGenreReste(y_resteKNN, seconde)\n",
    "\n",
    "print(100*np.sum(true_genre_reste[0:300]==genre_resteKNN[0:300])/300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix définitif entre un classifieur de type KNN et LDA pourrait constituer un travail d'approfondissement de ce projet. Le KNN peut offrir des performances supérieures, mais il faut prendre garde à éviter le surapprentissage. Il serait intéressant de les comparer sur l'ensemble de l'enregistrement afin de trancher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Dans ce TP nous avons pu créer et manipuler plusieurs caractéristiques issue d'un enregistrement sonore. La représentation temporelle d'un signal d'amplitude échantillonné, sa représentation spectrale grâce à la transformée de Fourrier, puis d'autres caractéristiques en vue d'une classification de séquences dans l'enregistrement comme le ZCR par une fenêtre glissante et les coefficients cepstraux à travers la représentation MFCC. D'un enregistrement sonore nous pouvons donc utiliser de nombreuses représentations en fonction de l'étude souhaitée in fine.\n",
    "\n",
    "Pour la classification des séquences nous avons utilisé une approche non supervisée par clustering et deux approches supervisée via un LDA et un KNN. Les trois méthodes ont donné des résultats satisfaisants et nous avons pu voir les avantages et limites de chacunes de ces méthodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
