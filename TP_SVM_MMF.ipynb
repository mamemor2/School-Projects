{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION DU MODULE 6 : SVM\n",
    "Mame-Mor Fall CES Data Science 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consignes : Vous rédigerez un compte-rendu détaillé de ce travail pratique (jusqu’à la question 11),\n",
    "incluant des réponses littérales, numériques et graphiques, ainsi que les implémentations Python réalisées.\n",
    "Votre compte-rendu prendra la forme d’un unique document linéaire IPython Notebook (.ipynb). Votre\n",
    "code devra être commenté de sorte à faire clairement apparaître les différentes parties et questions de ce\n",
    "travail pratique, ainsi que les opérations élémentaires réalisées dans vos implémentations. Le code Python\n",
    "devra être exécutable par le correcteur et vos résultats reproductibles. Ce compte-rendu est à envoyer\n",
    "par courriel à maxime.sangnier@telecom-paristech.fr au plus tard le lundi 20 mai 2016 à 23h59. Seuls\n",
    "les documents effectivement reçus avant ce moment seront corrigés et notés.\n",
    "\n",
    "## Questions\n",
    "\n",
    "### Approche intuitive\n",
    "1.Exécuter le script svm_gui.py. Il permet d’évaluer en temps réel l’impact du choix du noyau et du\n",
    "paramètre de régularisation C. Effectuer quelques tests (données linéairement séparables ou non,\n",
    "unimodales ou non, différents noyaux et paramètres) et commenter les observations. En particulier,\n",
    "pour des données unimodales (séparables puis avec recouvrement des classes), comment réagit\n",
    "le classifieur (frontière et marge) en fonction du choix de C d’abord (noyau linéaire) puis de γ\n",
    "(paramètre du noyau gaussien) ? Le choix de ces paramètres est-il crucial pour obtenir de bons taux\n",
    "de reconnaissance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données linéairement séparables :\n",
    "Impact du choix de C\n",
    "\n",
    "On observe que pour des données linéairement séparables le choix de C n'a aucun impact sur le positionnement des vecteurs support, ce qui est normal car C est un paramètre de \"régularisation\" qui agit comme un compromis biais-variance en pénalisant le choix de vecteurs support minimisant la fonction de coût mais trop \"éloigné\" des données d'apprentissage. Dans notre cas où les données sont parfaitement séparables nous n'avons pas ce problème.\n",
    "\n",
    "Impact du choix du noyau gaussien\n",
    "\n",
    "Le noyau gaussien fait partie d'un ensemble de techniques appellées \"kernel trick\" qui permet de transformer l'espace de réprésentation des données vers un espace de plus grande dimension où les données seraient linéairement séparables. Dans le cas du noyau gaussien, un espace de dimension infinie. Ici on observe que l'hyperplan séparateur n'est plus linéaire mais courbé autour des données pour les séparer en deux espaces. Les vecteurs supports passent par les données mais leur interprétation est moins évidente.\n",
    "\n",
    "Impact du choix du noyau polynomial\n",
    "\n",
    "Le noyau polynomial donne également un hyperplan séparateur et des vecteurs supports courbés qui passent bien par les points des deux classes les plus proches de l'hyperplan séparateur.\n",
    "\n",
    "Données linéairement non-séparables :\n",
    "Impact du choix de C\n",
    "\n",
    "On observe qu'avec des \"outliers\", des point des deux classes qui se situent au sein de l'autre classe (données non linéairement séparables), plus le terme de régularisation C est grand plus les vecteurs supports \"s'éloignent\" des données pour aller chercher les outliers. Ce qui correspond bien au comportement attendu.\n",
    "\n",
    "Impact du choix du noyau gaussien\n",
    "\n",
    "Ici le choix d'un noyau pour permettre de séparer les données prend tout son sens. Le noyau gaussien permet de s'affranchir de l'hyperplan séparateur linéaire pour obtenir un hyperplan courbé dans l'espace de réprésentation originel des données et sépare parfaitement les deux classes. Le choix du paramètre gamma permet d'ajuster la \"distance\" de l'hyperplan séparateur aux données. Gamma est un paramètre sur la distance euclidienne des données au séparateur de classe.\n",
    "\n",
    "Impact du choix du noyau polynomial\n",
    "\n",
    "Avec le noyau polynomial de degré 3, dans l'exemple que j'ai réalisé, les données sont bien séparées en deux classes et les vecteurs supports passent bien par les points les plus proches de l'hyperplan séparateur. Le noyau polynomial transforme l'espace originel des données en un espace contenant de nouvelles dimensions qui sont les produits croisés des features pour y trouver un hyperplan séparateur linéaire dans ce nouvel espace. Dans l'espace d'origine l'hyperplan séparateur est ainsi non linéaire. Dans notre cas les courbures de l'hyperplan sont difficiles à interpréter. On peut réduire le nombre de degré (donc la flexibilité) de l'hyperplan séparateur de 3 à 2 pour obtenir une frontière plus interprétable. Attention le choix de l'intercept aussi a une influence sur le l'hyperplan séparateur, dans ce cas, plus il sera élevé plus il sera prépondérant dans l'espace de dimension supérieur transformant le séparateur courbé en un quasi plan linéaire dans l'espace originel. \n",
    "\n",
    "On conclue donc que l'astuce du noyau est un outil puissant dans le cas de données non linéairement séparables mais personnellement mon opinion est que dans un soucis d'interprétabilité des résultats on sera attentif à son analyse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Estimer un classifieur linéaire sur des données unimodales avec recouvrement des classes puis rajouter\n",
    "des points un par un. Quelles sont les trois zones d’intérêt et comment réagit le classifieur lors\n",
    "de l’ajout d’un point dans l’une de ces zones ? Que dire de la variable duale αi associée à chaque\n",
    "point xi en fonction des zones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone 1 : Nouveau point derrière le vecteur support\n",
    "\n",
    "Le SVC ou classifieur à plus large marge par sa fonction de perte \"charnière\" ou hinge n'est pas modifié par tous les points qui seraient correctement classifié de fait (derrière le vecteur support). L'ajout d'un nouveau point \"correctement\" classifié n'a donc aucun impact sur le positionnement des vecteurs supports, ceci est une différence majeure par rapport à d'autres classifieurs. Aplha, solution du problème dual, est nul pour ces points.\n",
    "\n",
    "Zone 2 : Nouveau point entre le vecteur support et l'hyperplan séparateur\n",
    "\n",
    "Dans cette zone, on observe un repositionnement du vecteur support pour s'aligner sur l'hyperplan linéaire séparateur qui passe par les points des deux classes les plus proches. Alpha n'est pas nul pour ces points, ce qui entraîne une modification des vecteurs supports.\n",
    "\n",
    "Zone 3 : Nouveau point derrière le vecteur support opposé\n",
    "\n",
    "Lorsqu'un nouveau point est ajouté derrière le vecteur support opposé les données deviennent non linéairement séparables. C'est à ce moment qu'en fonction du paramètre de régularisation C, le SVC va repositionner les vecteurs supports pour réduire le \"coût\" payé pour la mauvaise classification de ce nouveau point. Plus le paramètre de régularisation C grand plus le SVC va déplacer les vecteurs supports pour diminuer ce coût."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Générer un jeu de données très déséquilibré (beaucoup plus de points dans une classe que dans\n",
    "l’autre). Avec un noyau linéaire, diminuer progressivement la valeur de C. Commenter. Ce phénomène\n",
    "peut être corrigé en pondérant d’avantage l’attache aux données sur la classe la moins présente\n",
    "(paramètre class_weight de sklearn.svm.SVC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, on observe que plus C est petit (paramètre de régularisation) plus les points appartenants aux données sont sur les vecteurs supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "4.À partir de la documentation : http://scikit-learn.org/stable/modules/svm.html, écrire un\n",
    "script estimant une SVM sur les classes 1 et 2 du jeu de donnée iris. Vous n’utiliserez que les deux\n",
    "premières variables et un noyau linéaire. Afficher le score moyen et la frontière de décision (utiliser\n",
    "les fonctions plot_2d et frontiere du fichier utils.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les données de la base de données Iris (dataset contenant 150x4 observations de trois différents types de fleur Iris les Setosa, les Versicolour et les Virginica décrites selon la longueur du sépale, sa largueur, la longueur du pétale et sa largueur).\n",
    "\n",
    "On importe également le classifieur et on oublie pas de normaliser les variables pour mieux prendre en compte toutes les variables.\n",
    "\n",
    "Ici on sépare l'ensemble des données en deux : 80% des données pour l'apprentissage du classifieur et 20% des données pour le test de validation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le score du classifieur sur les données de test est 0.766666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from math import fmod\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data[:,:2]\n",
    "y=iris.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_std=scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_stdtrain, X_stdtest, y_train, y_test = train_test_split(X_std,y,test_size=0.2,random_state=50)\n",
    "\n",
    "clf_SVC=LinearSVC()\n",
    "clf_SVC.fit(X_stdtrain,y_train)\n",
    "clf_SVCscore=clf_SVC.score(X_stdtest,y_test)\n",
    "print \"le score du classifieur sur les données de test est\",clf_SVCscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "########            Displaying labeled data                         ########\n",
    "############################################################################\n",
    "symlist = ['o', 's', 'D', '+', 'x',  '*', 'p', 'v', '-', '^','h','H']\n",
    "collist = ['blue', 'grey','red', 'purple', 'orange', 'salmon', 'black',\n",
    "           'fuchsia','chartreuse','burlywood']\n",
    "\n",
    "\n",
    "def plot_2d(data, y=None, w=None, alpha_choice=1):\n",
    "    \"\"\" Plot in 2D the dataset data, colors and symbols according to the\n",
    "    class given by the vector y (if given); the separating hyperplan w can\n",
    "    also be displayed if asked\"\"\"\n",
    "    if y is None:\n",
    "        labs = [\"\"]\n",
    "        idxbyclass = [range(data.shape[0])]\n",
    "    else:\n",
    "        labs = np.unique(y)\n",
    "        idxbyclass = [np.where(y == labs[i])[0] for i in range(len(labs))]\n",
    "\n",
    "    for i in range(len(labs)):\n",
    "        plt.plot(data[idxbyclass[i], 0], data[idxbyclass[i], 1], '+',\n",
    "                 color=collist[i % len(collist)], ls='None',\n",
    "                 marker=symlist[i % len(symlist)])\n",
    "    plt.ylim([np.min(data[:, 1]), np.max(data[:, 1])])\n",
    "    plt.xlim([np.min(data[:, 0]), np.max(data[:, 0])])\n",
    "    mx = np.min(data[:, 0])\n",
    "    maxx = np.max(data[:, 0])\n",
    "    if w is not None:\n",
    "        plt.plot([mx, maxx], [mx * -w[1] / w[2] - w[0] / w[2],\n",
    "                              maxx * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"g\", alpha=alpha_choice)\n",
    "\n",
    "def plot_2d_simple(data,y=None):\n",
    "    if y==None:\n",
    "        plt.scatter(data[:,0],data[:,1],s=50)\n",
    "    else:\n",
    "        nY=len(y)\n",
    "        Ycol=[collist[ y.astype(int)[i] -1 % len(collist)] for i in xrange(nY)]\n",
    "        plt.scatter(data[:,0],data[:,1],c=Ycol,s=40 )\n",
    "\n",
    "############################################################################\n",
    "########            Displaying tools for the Frontiere              ########\n",
    "############################################################################\n",
    "\n",
    "\n",
    "from matplotlib import colors\n",
    "cmap = colors.ListedColormap([ 'SteelBlue','DarkGray', 'LightSalmon', 'LightPink'])\n",
    "bounds=[0,1,2]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "\n",
    "def frontiere(f, data, step=50):\n",
    "    \"\"\" Plot the frontier of a decision function f\"\"\"\n",
    "    xmin, xmax = data[:, 0].min() - 1., data[:, 0].max() + 1.\n",
    "    ymin, ymax = data[:, 1].min() - 1., data[:, 1].max() + 1.\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, (xmax - xmin) * 1. / step),\n",
    "                         np.arange(ymin, ymax, (ymax - ymin) * 1. / step))\n",
    "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    #plt.imshow(z, origin='lower', interpolation=\"nearest\",extent=[xmin, xmax, ymin, ymax], cmap=cm.jet)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"nearest\",\n",
    "                extent=[xmin, xmax, ymin, ymax], cmap=cmap)  \n",
    "\n",
    "    plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD7CAYAAAClvBX1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04d64e9650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_2d(X_std,y)\n",
    "\"\"\"decisionf=clf_SVC.decision_function()\n",
    "frontiere(decisionf,X_std,step=50)\"\"\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.En laissant la moitié des données de côté, évaluer la performance en généralisation\n",
    "du modèle. Pour ce faire, vous déterminerez C par validation croisée en 5 étapes\n",
    "(scores = cross_val_score(clf, X, y, cv=5)). Comparer le résultat avec une SVM polynomiale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer le meilleur coefficient C de régularisation on utilise la fonction cross_val_score de la librairie cross_validation de sklearn. Celle-ci nous permet d'utiliser l'algorithme k-fold pour calculer le meilleur paramètre sur les données d'apprentissage. Attention, la validation croisée n'est effectuée que sur les données d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.80222965440356742, 100: 0.78455964325529537, 5: 0.80992196209587508, 1000: 0.70738573021181717, 10: 0.80992196209587508, 15: 0.80992196209587508, 10000: 0.77486064659977705}\n",
      "le score obtenu le plus élevé pour un noyau linéaire est 0.809921962096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = {}\n",
    "for c in [1,5,10,15,100,1000,10000] :\n",
    "    clf_SVC=LinearSVC(C=c)\n",
    "    scores[c] = np.mean(cross_val_score(clf_SVC, X_stdtrain, y_train, cv=5, scoring='accuracy'))\n",
    "print(scores)\n",
    "print \"le score obtenu le plus élevé pour un noyau linéaire est\",max(scores.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise la même opération avec un classifieur SVC utilisant un noyau polynomial. On ne joue ici que sur C et pas sur le degré du noyau polynomial (degré 3 par défaut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.6991360089186176, 100: 0.72686733556298766, 5: 0.70947603121516167, 1000: 0.72686733556298766, 10: 0.69208472686733546, 15: 0.69208472686733546, 10000: 0.72686733556298766}\n",
      "le score obtenu le plus élevé pour un noyau polynomial est 0.726867335563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = {}\n",
    "for c in [1,5,10,15,100,1000,10000] :\n",
    "    clf_SVC=SVC(C=c,kernel='poly')\n",
    "    scores[c] = np.mean(cross_val_score(clf_SVC, X_stdtrain, y_train, cv=5, scoring='accuracy'))\n",
    "print(scores)\n",
    "print \"le score obtenu le plus élevé pour un noyau polynomial est\",max(scores.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'aperçoit que le score obtenu est plus faible, ce qui signifie que le modèle polynomial se généralise moins bien que le modèle linéaire sur des nouvelles données. On constate à nouveau que plus C est élevé meilleur sont les résultats, ce qui est conforme à nos attentes car le modèle s'attache moins aux données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.En vous inspirant de l’exemple http://scikit-learn.org/stable/auto_examples/svm/plot_\n",
    "rbf_parameters.html (mais sans utiliser MidpointNormalize), afficher une carte de performance\n",
    "du noyau gaussien appliqué aux données d’apprentissage (on utilisera que 13 valeurs en base 2 pour\n",
    "C et γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Dans la suite, on s’intéresse à un problème de classification de visages. À partir du script\n",
    "svm_lfw.py, montrer l’influence du paramètre de régularisation. On pourra par exemple afficher\n",
    "l’erreur de prédiction (en test) en fonction de C sur une échelle logarithmique entre 1e-6 et 1e3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Le script que vous utilisez centre et normalise les données. Décrire comment et expliquer l’intérêt\n",
    "de cette opération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. En conservant les données précédemment créées et pour différentes tailles de l’ensemble d’apprentissage,\n",
    "évaluer la performance en généralisation d’une SVM linéaire (pour ce faire, choisir C par\n",
    "validation croisée et enregistrer le score en test). Tracer la courbe d’apprentissage (i.e. le score en\n",
    "fonction de la taille de l’ensemble d’apprentissage). Cette dernière est une approche empirique de\n",
    "la consistance de notre estimateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. L’exemple http://scikit-learn.org/stable/auto_examples/svm/plot_separating_\n",
    "hyperplane.html#example-svm-plot-separating-hyperplane-py explique comment accé-\n",
    "der au paramètres estimés lors de l’apprentissage : vecteur de coefficients w dans l’attribut\n",
    "coef_, w0 enregistré dans l’attribut intercept, liste des vecteurs de supports, coefficients du\n",
    "problème dual). À partir de cet exemple, écrire un script qui calcule la valeur des fonctionnelles\n",
    "primale et duale. Vérifier que les valeurs sont proches (attention, les étiquettes doivent être −1\n",
    "ou 1). Comment varie la différence entre les deux valeurs quand on fait varier la tolérance sur\n",
    "l’optimisation (paramètre tol de SVC) ?\n",
    "Régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. On s’intéresse à présent à prédire l’activité d’une molécule. Pour ce faire, nous considérons une\n",
    "molécule comme un graphe étiqueté, représenté par un ensemble de relations entre ses nœuds (les\n",
    "atomes de la molécule). En suivant ces relations au sein d’une molécule x, on parcourt un chemin p\n",
    "dans le graphe correspondant. Soit alors Pd l’ensemble des chemins possibles (de longueur inférieure\n",
    "à d) pour la famille de graphes considérée. On note ı(x, p), l’indicateur valant 1 si le chemin p est\n",
    "présent dans le graphe x, et 0 sinon. \n",
    "À partir du fichier drug_activity.py 1, donner le meilleur score possible en prédiction sur le jeu de\n",
    "données test à l’aide d’une machine à vecteurs supports. Les données de test ne doivent pas intervenir\n",
    "dans l’apprentissage. Comparer avec la régression régularisée. Changer la mesure d’erreur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Classification de textes###\n",
    "12. Travailler sur le tutoriel : http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
